\documentclass[11pt]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumerate}
\usepackage{fullpage}
\begin{document}
\title{LAB 3}
\author{Jack Sullivan and Ari Kobren}
\maketitle

\section{Main Components}
\label{sec:main}
Like the last assignment, our architecture revolved around three
components: a \emph{DBServer} and 2 \emph{FrontendServer}s.  However,
to manage fault tolerance, we also implemented a
\emph{FrontendManager} that helped keep track of which servers were
up.

\subsection{FrontendManager}
\label{subsec:manager}
Unlike in the previous assignment, we no longer have a
\emph{RequestRouter} that evenly distributes requests made by
\emph{TableClient}s.  Instead, we have implemented a
\emph{FrontendManager} that watches both \emph{FronendServer}s in case
they go down and also assigns \emph{TabletClient}s to communicate with
particular servers.  Specifically, both \emph{FrontendServer}s
repeatedly send the \emph{FrontendManager} \emph{Heartbeat} messages
at a prespecified interval.  Every interval, the
\emph{FrontendManager} scans the \emph{Heartbeat} messages it has
recently received to ensure that the \emph{FrontendServer}s are
alive.  If it hasn't received a message from a \emph{FrontendServer},
it assumes that server is dead and diverts all clients of that server
to the other living server.  Once the dead server comes back online,
the \emph{FrontendManager} reassigns clients to that server.

\subsection{Caching Frontend}
\label{subsec:caching}
A \emph{CachingFrontend} describes a modification we've made to our
existing \emph{FrontendServer} object. As the name suggests, the
\emph{CachingFrontend} extends a server by allowing it to cache by
either using pull or push based approaches.  For both the push and
pull based communication we implemented caching in both of the
\emph{FrontendServer}s.  In our implementation, we keep tables of the
most recently queried scores and medal tallys.  This way, when a
client makes a query, we can respond quickly if the information is in
the table.  In both push and pull based caching, this is implemented
by listening for \emph{DBResponse} messages from the \emph{DBServer}.
Upon receiving a \emph{DBResponse} each \emph{FrontendServer} stores
the response (either a medal tally or event score) in its cache and
forwards the message onto the appropriate client.

\subsubsection{Push Based Caching}
\label{subsubsec:push}
In push based caching, each time a \emph{FrontendServer} receives an
update from \emph{Cacofonix} it forwards the message to the
\emph{DBServer} and sends an \emph{InvalidateEvent} or an
\emph{InvalidateTeam} message to the \emph{FrontendManager}.  When the
\emph{FrontendManager} receives one of these messages, it broadcasts
the message to both of the \emph{FrontendServer}s (this is so that if
there were more than 2 \emph{FrontendServer}s our system would still
operate appropriately).  When the \emph{FrontendServer} receives the
\emph{InvalidateEvent} or \emph{InvalidateTeam} message, it clears the
corresponding entry in its cache.

\subsubsection{Pull Based Caching}
\label{subsubsec:pull}
In pull based caching, each \emph{FrontendServer} is responsible for
keeping its cache up to date.  To do this, each \emph{FrontendServer}
periodically sends an \emph{InvalidateCache} message to the
\emph{FrontendManager} which, in turn, broadcasts the
\emph{InvalidateCache} message to all \emph{FrontendServer}s.  When a
\emph{FrontendServer} receives an invalidate cache message, it clears
its medal cache and its event cache.  When the cache is cleared, each
client query will require the \emph{FrontendServer} to query the
\emph{DBServer} and repopulate its own cache.

\subsection{Fault Tolerance}
We have also implemented some fault tolerant characteristics which are
programatically separated from caching but, in implementation, get
mixed into the frontend.  The fault tolerance characteristic requires
a frontend to periodically send heartbeat messages to the
\emph{FrontendManager}.

\subsection{TabletClients and Cacofonix}
\label{subsec:client}
As before, \emph{TabletClient}s can connect to the system and request
a team's medal tally and also the score of an ongoing event.  However,
now, when a client wants to begin receiving updates, it must send a
\emph{Register} message to the \emph{FrontendManager}.  When the
manager receives this message, it allocates the client to a particular
\emph{FrontendServer} and sends the client that server's address.
From then on, the client requests scores from that server.  This
communication mechanism with the manager is important for fault
tolerance because if the server a client is communicating with goes
down, the \emph{FrontendManager} will know and send the client a new
frontend with which to communicate.

\emph{Cacofonix} must also register with the \emph{FrontendManager} to
determine which frontend it will be communicating with. If the server
\emph{Cacofonix} is communicating with goes down this mechanism will
send \emph{Cacofonix} a new frontend with which to communicate so that
this communication is fault tolerant.

\subsection{Previously Built Components}
We were able to incorporate components (messages and classes) that we
built for Lab 1 and Lab 2.  Specifically, we used (and slightly
modified): \emph{EventRoster}, \emph{TeamRoster}, \emph{Event},
\emph{EventMessage}, \emph{EventSubscription}, \emph{MedalTally},
\emph{TabletClients} and \emph{EventScore}.

\section{System Runtime Workflow}
We start our system by starting up the \emph{DBServer}, the
\emph{FrontendManager} and two \emph{FrontendServer}s as seperate
processes.  We also start a process for \emph{Cacafonix} and $n$
\emph{TabletClients}.  These clients are responsible for sending
registration requests to the \emph{FrontendManager} and then being
assigned a frontend with which to communicate (Section
~\ref{subsec:client}).

Clients periodically send \emph{EventScore} and \emph{MedalTally}
requests as in the previous assignment. \emph{FrontendServer}s respond
to their respective clients using a caching strategy that was
specified via the command line (Section ~\ref{subsec:caching}). When a
client receives a response, it prints that response to the screen.

We also bring our \emph{FrontendServer}s down periodically to test
faul tolerance. This operates using \emph{Heartbeat} messages and
communication with the \emph{FrontendManager} (Section
~\ref{subsec:manager}).

\emph{Cacofonix} sends score and medal tally updates just like in the
previous assignments.

\section{Design, Bottlenecks and Potential Improvements}
One bottleneck that we've identified in our system is the
\emph{FrontendManager}.  The probelm with this object is that it is
soley responsible for the two \emph{FrontendServer}s.  If the
\emph{FrontendManager} were to go down, there would be nothing keeping
track of the status of the two \emph{FrontendServers} (meaning that we
would run into trouble if one of them went down).

There are two ways we could fix this problem. First, we could make
each \emph{FrontendServer} extend \emph{FrontendManager}.
Conceptually this would translate into 3 \emph{FrontendServers} one of
which acts as the \emph{FrontendManager} and the other two acting as
\emph{FrontendServer}s.  Then, if any of the servers went down, our
system could respawn that server and switch the managerial role if
necessary.  We could use some of the logic we implemented in the last
assignment (e.g. leader election) to pick which of the three servers
would be the manager.

A simpler solution would be to make use of Akka's \emph{Router}
class.  This class handles fault tolerance by default.  The reason we
didn't use this class is that we wanted to implement fault tolerance
ourselves.  In a production setting, we'd probably make heavy use of
the \emph{Router} class.

Like last time, our \emph{FrontEndServer}s and \emph{FrontendManager}
are single threaded.  When the system is heavily loaded, this could
become problematic and increase average response time.  One way to
alleviate this problem would be to multithread our servers or make use
of Akka \emph{Router}s which operate asynchronously by default.

Another potential bottleneck in our system is our method of
load-balancing.  As the \emph{FrontendManager} receives

\section{Results}
We ran our code using 5 clients using 10, 100 and 500 milliseconds
between requests and measured the min, max and average response
times when using \emph{push based caching}.

\begin{tabular}{c|c|c|c}
  MS BTW REQ & MIN & MAX & AVG \\
  \hline
  10  & 0.1s  & 1.42s & 0.9130s \\%& 0.07s & 1.44s & 0.99739s \\
  100 & 0.01s & 0.08s & 0.0195s \\%& 0.01s & 10.2s & 4.94568s \\
  500 & 0.01s & 0.05s & 0.0106s \\%& 0.01s & 50.6s & 24.4193s \\
\end{tabular}

Next, we repeated the experiment when using \emph{pull based caching}.

\begin{tabular}{c|c|c|c}
  MS BTW REQ SEC & MIN & MAX & AVG \\
  \hline
  10  & 0.5s  & 1.57s & 1.25925s \\
  100 & 0.01s & 0.07s & 0.01312s \\
  500 & 0.01s & 0.06s & 0.01376s \\
\end{tabular}

In our final test, we ran our system while killing a random
\emph{FrontendServer} every 2 seconds.  For this experiment we use
push based caching.

\begin{tabular}{c|c|c|c}
  REQ Per SEC & MIN & MAX & AVG \\
  \hline
  500  & 0.01s  & 0.05s & 0.026s \\
\end{tabular}

For push-based caching, we noticed that our system performs more
slowly when it's under higher load (clients make requests more
frequently).  This is expected.  It is interesting to note that the
average latency drops off a tremendous amount when there are fewer
requests per second.  However, at some point, it seems like there is some
floor latency (around 0.01s) inherent in the system.

In pull-based caching, we notice the same trend (which is expected).
However, the average latency when requests are made very
frequently is higher than in pull-based caching.  We believe this is
the case because in pull-based caching, updates invalidate the whole
cache leading to more cache misses than in push-based caching.  This,
in turn, makes pull-based caching slower. It is also possible that, 
since our actors process messages serially, that cache invalidation 
messages, which are relatively frequent, are adding load to the 
frontend servers which causes additional latency. We also believe that we
have noticed a similar latency floor for pull-based caching around
0.013 seconds (this is again slower than push-based caching).

In a real system, we would probably implement push based caching or a
smarter version of pull-based caching that didn't invalidate the whole
cache when updating.

In the `faulty' system experiment, we observe, as expected that the 
latency is higher across the board. This is likely due to a combination 
of additional message overhead from reassigning work to living servers 
and restarting them, as well as the additional client message volume that 
the living server needs to handle.

%For our final test we simulated a faulty frontend network by killing a random frontend server every 

%% As we observe, adding more clients seems to increase average latency.
%% There is a significant difference in average latency (2.4x) when we
%% increase from 5 to 10 clients.  there is less of a relative difference
%% when we increase to 10 clients. Surprisingly, the minimum latency
%% decreases in these runs. We hypothesize that this could be due to
%% other things occuring on the network while running our tests or some
%% noise related to our leader elections.

%% Also, we observe that having the clients make requests at higher
%% frequencies severely affects the latency.  As we flood the network
%% with more requests, things drastically slow down.  When we cut our
%% between request wait time by 5 (from .5s to .1s) we experience a 3x
%% average latency increase (3x slower to get a response).  When we again
%% reduce our wait time by a factor of 10 (from .1s to .01s) we see a
%% 5.5x slow down.

\section{Software}
Like Lab 1 and 2, we've made extensive use of the Akka \emph{actors}
library.  This library provides a hierarchical message passing
architecture for the Scala programming language.

\end{document}
